# Personal Assistant CLI Development Prompt

## Project Overview
Create a command-line personal assistant named "assistant" that integrates AWS Strands Agent SDK with a Python MCP memory server to provide persistent memory capabilities across conversations. The memory server is just one MCP server - the architecture should support additional MCP servers in the future.

## Core Requirements

### 1. Technology Stack
- **Primary Framework**: AWS Strands Agent SDK (latest version)
- **Memory System**: Python MCP Memory Server from https://github.com/jason-c-dev/memory-mcp-server-py
- **Python Version**: 3.10+
- **Interface**: Command-line based (non-interactive initially)

### 2. Architecture Components

#### **Configuration System**
Create a flexible configuration file (`config.yaml`) that supports multiple model providers and MCP servers:

```yaml
# Default provider (uses Strands built-in model providers)
default_provider: "auto"  # auto-detect based on env vars, or specify: anthropic, openai, bedrock, ollama

# Model provider configurations (using Strands built-in providers)
# Default model: Claude 3.7 Latest across all providers
providers:
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    model_id: "claude-3-7-sonnet-20250219"  # Anthropic API
    temperature: 0.7
    max_tokens: 4096
  
  openai:  # via LiteLLM
    api_key: "${OPENAI_API_KEY}"
    model_id: "gpt-4"
    temperature: 0.7
    max_tokens: 4096
  
  bedrock:  # Default in Strands
    region_name: "us-west-2"
    model_id: "us.anthropic.claude-3-7-sonnet-20250219-v1:0"  # AWS Bedrock
    temperature: 0.7
    # Uses boto3 credential chain automatically
  
  ollama:  # For local development
    base_url: "http://localhost:11434"
    model_id: "llama3.3"
    temperature: 0.7

# MCP server configurations
mcp_servers:
  memory:
    command: "python"
    args: ["./memory_server/mcp_memory_server.py"]
    env:
      MEMORY_FILE_PATH: "~/.assistant/memory.json"
    required: true  # This server is required for the assistant to function
  
  # Example of additional MCP servers (for future expansion)
  # filesystem:
  #   command: "npx"
  #   args: ["@modelcontextprotocol/server-filesystem", "/path/to/allowed/files"]
  #   required: false

# System prompt for effective memory integration
system_prompt: |
  You are a helpful personal assistant with persistent memory capabilities. Follow these guidelines:
  
  1. **User Identification**: Assume you're interacting with the default user. If you haven't identified them, proactively try to do so.
  
  2. **Memory Retrieval**: Always begin interactions by saying "Remembering..." and retrieve relevant information from your knowledge graph.
  
  3. **Memory Categories**: Pay attention to new information in these categories:
     - Basic Identity (age, gender, location, job, education, etc.)
     - Behaviors (interests, habits, preferences, etc.) 
     - Goals (targets, aspirations, projects, etc.)
     - Relationships (personal and professional up to 3 degrees)
     - Important Events (meetings, deadlines, milestones, etc.)
  
  4. **Memory Updates**: After each interaction, update your memory by:
     - Creating entities for recurring people, organizations, and significant events
     - Connecting them with appropriate relations
     - Storing relevant facts as observations
  
  5. **Conversation Style**: Be conversational, helpful, and reference past interactions when relevant.

# CLI settings  
cli:
  verbose: false
  log_level: "INFO"
  output_format: "text"  # text, json, markdown
```

#### **MCP Integration Architecture**
Following Strands best practices for MCP integration:

- **Use Strands' built-in MCP support**: Leverage `MCPClient` from `strands.tools.mcp` with `stdio_client` and `StdioServerParameters`
- **Stdio-based servers**: Connect to MCP servers using stdio transport, which runs servers in dedicated threads
- **No direct MCP interaction**: All communication with MCP servers goes through Strands' MCP protocol implementation
- **Context management**: Use context managers (`with mcp_client:`) to properly manage server lifecycle

```python
from mcp import stdio_client, StdioServerParameters
from strands import Agent
from strands.tools.mcp import MCPClient

# Setup memory MCP client (Strands handles threading and protocol)
memory_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(
        command="python",
        args=["./memory_server/mcp_memory_server.py"],
        env={"MEMORY_FILE_PATH": memory_file_path}
    ))
)

# Create agent with memory tools
with memory_client:
    tools = memory_client.list_tools_sync()
    agent = Agent(
        model=selected_model,
        tools=tools,
        system_prompt=system_prompt
    )
```

#### **CLI Interface Design**
Following the Strands CLI reference pattern:

**Basic Usage**:
```bash
# Single query mode
assistant "What meetings do I have this week?"

# Help and version
assistant --help
assistant --version

# Configuration options
assistant --config /path/to/config.yaml "Tell me about my recent projects"
assistant --provider anthropic "What did I discuss with John last week?"
assistant --verbose "Summarize my goals for this quarter"
```

**Command Arguments**:
- `query`: The question/command for the assistant
- `--config, -c`: Path to configuration file (default: `~/.assistant/config.yaml`)
- `--provider, -p`: Override model provider (anthropic, openai, bedrock, ollama)
- `--verbose, -v`: Enable verbose output showing memory operations
- `--reset-memory`: Clear all stored memory (with confirmation)
- `--export-memory`: Export memory to JSON file
- `--import-memory`: Import memory from JSON file

### 3. Implementation Structure

```
assistant/
├── assistant/
│   ├── __init__.py
│   ├── cli.py              # Main CLI interface using argparse/click
│   ├── config.py           # Configuration management & provider detection
│   ├── agent.py            # Strands agent setup with MCP integration
│   └── providers.py        # Provider detection and setup using Strands built-ins
├── memory_server/          # Git submodule or copy of memory MCP server
│   ├── mcp_memory_server.py
│   ├── requirements.txt
│   └── ...
├── config.yaml             # Default configuration file
├── requirements.txt        # Project dependencies
├── setup.py               # Package setup
└── README.md              # Installation and usage guide with 30sec setup
```

### 4. Key Implementation Details

#### **Provider Setup Using Strands Built-ins**:
```python
from strands import Agent
from strands.models import BedrockModel, AnthropicModel, OllamaModel
from strands.models.litellm import LiteLLMModel

def setup_model(provider_config, provider_type):
    """Setup model using Strands built-in providers"""
    if provider_type == "anthropic":
        return AnthropicModel(
            model_id=provider_config["model_id"],
            api_key=provider_config["api_key"],
            temperature=provider_config.get("temperature", 0.7)
        )
    elif provider_type == "bedrock":
        return BedrockModel(
            model_id=provider_config["model_id"],
            region_name=provider_config.get("region_name", "us-west-2"),
            temperature=provider_config.get("temperature", 0.7)
        )
    elif provider_type == "ollama":
        return OllamaModel(
            host=provider_config["base_url"],
            model_id=provider_config["model_id"],
            temperature=provider_config.get("temperature", 0.7)
        )
    elif provider_type == "openai":
        return LiteLLMModel(
            model_id=provider_config["model_id"],
            api_key=provider_config["api_key"],
            temperature=provider_config.get("temperature", 0.7)
        )
```

#### **Auto-Detection Logic**:
```python
def detect_provider():
    """Auto-detect available provider based on environment variables"""
    if os.getenv('ANTHROPIC_API_KEY'):
        return 'anthropic'
    elif os.getenv('OPENAI_API_KEY'):
        return 'openai'  
    elif os.getenv('AWS_ACCESS_KEY_ID') or boto3.Session().get_credentials():
        return 'bedrock'
    else:
        return 'ollama'  # fallback to local
```

#### **MCP Integration Following Strands Patterns**:
```python
from mcp import stdio_client, StdioServerParameters
from strands import Agent
from strands.tools.mcp import MCPClient

# Setup memory MCP client following Strands documentation
memory_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(
        command="python",
        args=["./memory_server/mcp_memory_server.py"],
        env={"MEMORY_FILE_PATH": os.path.expanduser("~/.assistant/memory.json")}
    ))
)

# For future: adding additional MCP servers (example from Strands docs)
# filesystem_client = MCPClient(
#     lambda: stdio_client(StdioServerParameters(
#         command="npx",
#         args=["@modelcontextprotocol/server-filesystem", "/allowed/path"]
#     ))
# )

# Create agent with MCP tools (single server)
with memory_client:
    tools = memory_client.list_tools_sync()
    agent = Agent(
        model=selected_model,
        tools=tools,
        system_prompt=system_prompt
    )

# For multiple MCP servers, use Strands recommended pattern:
# with memory_client, filesystem_client:
#     tools = memory_client.list_tools_sync() + filesystem_client.list_tools_sync()
#     agent = Agent(model=selected_model, tools=tools, system_prompt=system_prompt)
```

### 5. Installation & Setup Requirements

#### **30-Second Install Guide** (in README.md):

```markdown
## 30-Second Install

1. **Setup virtual environment:**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   pip install -e .
   ```

3. **Setup memory server:**
   ```bash
   git submodule update --init --recursive
   cd memory_server && pip install -r requirements.txt && cd ..
   ```

4. **Create config directory:**
   ```bash
   mkdir -p ~/.assistant
   cp config.yaml ~/.assistant/config.yaml
   ```

5. **Set API key and test:**
   ```bash
   export ANTHROPIC_API_KEY="your-key-here"  # or set in config
   assistant "Hello, I'm testing the setup"
   ```
```

#### **Dependencies** (`requirements.txt`):
```
strands-agents>=0.1.0
strands-agents-tools>=0.1.0
click>=8.0.0
pyyaml>=6.0
# No need for individual provider packages - Strands includes them
```

### 6. Memory Configuration Clarification

The memory file path is NOT hardcoded but properly configured through:

1. **MCP Server Environment Variable**: The memory server uses `MEMORY_FILE_PATH` environment variable to set storage location
2. **Configuration File**: Default path `~/.assistant/memory.jsonl` set in config.yaml
3. **CLI Override**: Optional `--memory-path` argument for custom locations
4. **Auto-creation**: Directory and file created automatically if they don't exist

```python
# Memory file configuration
memory_path = config['mcp_servers']['memory']['env']['MEMORY_FILE_PATH']
memory_path = os.path.expanduser(memory_path)  # Handle ~ expansion
os.makedirs(os.path.dirname(memory_path), exist_ok=True)
```

### 7. Model Selection Rationale

**Default Model: Claude 3.7 Latest**
- Anthropic API: `claude-3-7-sonnet-20250219` 
- AWS Bedrock: `us.anthropic.claude-3-7-sonnet-20250219-v1:0`
- OpenAI: `gpt-4` (latest available)
- Ollama: `llama3.3` (local development)

### 8. Success Criteria

**Functional Requirements**:
- ✅ Single command execution with persistent memory via MCP
- ✅ Auto-detection of available model providers using Strands built-ins
- ✅ Proper MCP integration following Strands patterns
- ✅ Configurable system prompts and model settings
- ✅ Extensible MCP server architecture

**User Experience**:
- ✅ Zero-config startup with sensible defaults
- ✅ Clear 30-second setup instructions
- ✅ Proper error handling for missing dependencies/keys
- ✅ Verbose mode for debugging operations

**Technical Quality**:
- ✅ Use Strands built-in providers (no custom provider code)
- ✅ Follow Strands MCP integration patterns
- ✅ Clean separation of concerns
- ✅ Type hints and documentation

### 9. Example Usage Scenarios

```bash
# First run - auto-detects Anthropic key, sets up memory MCP server
assistant "I'm starting a new project called WebStore using React and Node.js"

# Later conversation - memory retrieved via MCP protocol
assistant "What technologies am I using for the WebStore project?"

# Add personal information
assistant "I live in San Francisco and work as a senior developer at TechCorp"

# Query relationships and context
assistant "Remind me about my current projects and where I work"

# Different provider
assistant --provider ollama "What have I been working on lately?"

# Verbose mode to see MCP operations
assistant --verbose "Update my skills to include Python and FastAPI"
```

## Development Approach

1. **Start Simple**: Basic CLI with Bedrock default and memory MCP integration
2. **Add Configuration**: Implement config file and provider auto-detection using Strands built-ins
3. **Polish UX**: Error handling, logging, and installation improvements
4. **Extend MCP**: Add support for additional MCP servers beyond memory

**Key Architectural Principles**:
- **Use Strands built-ins**: No custom provider implementations needed
- **Follow MCP patterns**: Use MCPClient with stdio transport as shown in Strands documentation
- **Extensible design**: Support multiple MCP servers from day one
- **Production ready**: Proper error handling, logging, and configuration management

Focus on leveraging Strands' existing capabilities rather than rebuilding functionality that's already provided by the SDK.